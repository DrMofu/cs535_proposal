%
% File acl2015.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
% \usepackage{cite}  


%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Project Proposal: Multimodal sentiment analysis with Graph Convolutional network }

\author{
  Wanrong Zheng,
  Xinwei Du,
  Haosheng Wang
  }
  
\date{\today}

\begin{document}
\maketitle
% \begin{abstract}
%   This document contains the instructions for preparing a camera-ready
%   manuscript for the proceedings of ACL-2015. The document itself
%   conforms to its own specifications, and is therefore an example of
%   what your manuscript should look like. These instructions should be
%   used for both papers submitted for review and for final versions of
%   accepted papers.  Authors are asked to conform to all the directions
%   reported in this document.
% \end{abstract}

\section{Problem description}
Multimodal sentiment analysis is the task of performing sentiment analysis given multiple data sources (vision, audio, language).
A good understanding of this task can help us better understand human social behavior and benefit human-computer interaction since human communication is multimodal and emotional.
Our research topic is highly related to our course \textit{Multimodal Probabilistic Learning of Human Communication}, since sentiment is an important part of human communication, and multimodal data processing is crucial for our task.

\section{Dataset}

Considering that the multimodal model can perform sentiment analysis from different perspectives such as images, sounds, and texts, the following datasets will be more in line with the requirements of this paper.

\subsection{CMU-MOSEI}
CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset is the largest dataset of multimodal sentiment analysis and emotion recognition to date. The dataset contains more than 23,500 sentence utterance videos from more than 1000 online YouTube speakers. The dataset is gender balanced. All the sentences utterance are randomly chosen from various topics and monologue videos. The videos are transcribed and properly punctuated.

\subsection{SEWA}
SEWA DB is a rich database for audio-visual emotion and sentiment research in the wild. With over 530 sentence utterance videos from 408 videos. Obtaining emotion data may help when finding out the relationship between GCN results.

\section{Method}


\cite{yin2021contrastive}
\section{Team member contributions}


% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{ref}


\end{document}