%
% File acl2015.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
% \usepackage{cite}  


%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Combining Graph Convolutional Network and Transformers for Multimodal Sentiment Analysis}

\author{
  Wanrong Zheng,
  Xinwei Du,
  Haosheng Wang
  }
  
\date{\today}

\begin{document}
\maketitle
% \begin{abstract}
%   This document contains the instructions for preparing a camera-ready
%   manuscript for the proceedings of ACL-2015. The document itself
%   conforms to its own specifications, and is therefore an example of
%   what your manuscript should look like. These instructions should be
%   used for both papers submitted for review and for final versions of
%   accepted papers.  Authors are asked to conform to all the directions
%   reported in this document.
% \end{abstract}

\section{Problem description}
Multimodal sentiment analysis is the task of performing sentiment analysis given multiple data sources (vision, audio, language).
A good understanding of this task can help us better understand human social behavior and benefit human-computer interaction since human communication is multimodal and emotional.
Our research topic is highly related to our course \textit{Multimodal Probabilistic Learning of Human Communication}, since sentiment is an important part of human communication, and multimodal data processing is crucial for our task.

\section{Dataset}

Considering that the multimodal model can perform sentiment analysis from different perspectives such as images, sounds, and texts, the following datasets will be more in line with the requirements of this paper.

\subsection{CMU-MOSEI}
CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset is the largest dataset of multimodal sentiment analysis and emotion recognition to date. The dataset contains more than 23,500 sentence utterance videos from more than 1000 online YouTube speakers. The dataset is gender balanced. All the sentences utterance are randomly chosen from various topics and monologue videos. The videos are transcribed and properly punctuated.

\subsection{SEWA}
SEWA DB is a rich database for audio-visual emotion and sentiment research in the wild. With over 530 sentence utterance videos from 408 videos. Obtaining emotion data may help when finding out the relationship between GCN results.

\section{Method}
\subsection{Related Work}
In the previous research, \cite{DBLP:journals/corr/abs-2006-15955} proposes a state-of-the-art approach to this topic. This approach describes a Transformer-based\cite{DBLP:journals/corr/VaswaniSPUJGKP17} joint-encoding for the task of Emotion Recognition and Sentiment Analysis. 
% This approach uses a new encoding architecture that fully eschews recurrence for sequence encoding and instead relies entirely on an attention mechanism and Feed-Forward Neural Networks to draw global dependencies between input and output. Its results can compare with, and sometimes surpass, the current state-of-the-art for both tasks on the CMU-MOSEI dataset.
However, such a network directly computes each task separately and combine them to a joint-encoding which ignores the connections between multimodel. Therefore, we are going to build a model which combines Graph Convolutional Network\cite{DBLP:journals/corr/KipfW16} and Transformers encoding.

\subsection{Model}
This network is consist of two parts, the first module is goint to extract feature using Transformer or Swin transformer\cite{DBLP:journals/corr/abs-2103-14030}. The second module will use the first module's output as input as a input of following Graph Convolutional Network. Also we will try to add Dynamic Fusion Graph\cite{bagher-zadeh-etal-2018-multimodal} structure to combine the multimodel feature.

\subsection{Evaluation Criteria}
Accuracy and F1 scores will be utilized to measure results on the private test-fold for 7-class sentiment problem and for each emotion.
\section{Team member contributions}
Wanrong Zheng will implement the swin transformer structure instead of the original transformer backbone. Xinwei Du will add Graph Convolutional Network to baseline. Haosheng Wang will add Dynamic Fusion Graph to our network.


% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{ref}


\end{document}